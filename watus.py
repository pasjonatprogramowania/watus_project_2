#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os

os.environ.setdefault("KMP_DUPLICATE_LIB_OK", "TRUE")
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("CT2_SKIP_CONVERTERS", "1")  # <— kluczowe: pomija import transformers w ctranslate2

import sys, json, time, queue, threading, subprocess, tempfile, atexit, re
from pathlib import Path
from collections import deque
from dotenv import load_dotenv
import numpy as np
import sounddevice as sd
import soundfile as sf
import zmq
import webrtcvad

# === ASR: Choose between Faster-Whisper (local) or Groq API ===
from faster_whisper import WhisperModel
from groq_stt import GroqSTT

load_dotenv(dotenv_path=Path(__file__).with_name(".env"), override=True)


# ===== TTS: Piper lub Gemini =====
TTS_PROVIDER = os.environ.get("TTS_PROVIDER", "gemini").lower()

try:
    from piper import PiperVoice
    PIPER_AVAILABLE = True
except ImportError:
    PIPER_AVAILABLE = False
    print("[Watus][TTS] Piper Python API nie dostępne, używam binary metod", flush=True)

# Gemini TTS
try:
    from google import genai
    from google.genai import types
    import mimetypes
    import struct

    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("[Watus][TTS] Google Gemini TTS nie dostępne - pip install google-genai", flush=True)

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
GEMINI_MODEL = os.environ.get("GEMINI_MODEL", "gemini-2.0-flash-exp")
GEMINI_VOICE = os.environ.get("GEMINI_VOICE", "Callirrhoe")

# === Kontroler diody LED ===
from led_controller import LEDController

# .env z katalogu pliku (i override, gdy IDE ma własne env)
load_dotenv(dotenv_path=Path(__file__).with_name(".env"), override=True)

# ===== ZMQ =====
PUB_ADDR = os.environ.get("ZMQ_PUB_ADDR", "tcp://127.0.0.1:7780")  # Watus:PUB.bind (dialog.leader/unknown_utterance)
SUB_ADDR = os.environ.get("ZMQ_SUB_ADDR", "tcp://127.0.0.1:7781")  # Watus:SUB.connect (tts.speak)


# ===== Whisper / Piper =====
def _normalize_fw_model(name: str) -> str:
    name = (name or "").strip()
    short = {"tiny", "base", "small", "medium", "large", "large-v1", "large-v2", "large-v3"}
    if "/" not in name and name.lower() in short:
        return f"guillaumekln/faster-whisper-{name.lower()}"
    return name


# ===== STT Configuration =====
STT_PROVIDER = os.environ.get("STT_PROVIDER", "local").lower()  # "local" for Faster-Whisper, "groq" for Groq API

# Groq API configuration (used when STT_PROVIDER=groq)
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
GROQ_MODEL = os.environ.get("GROQ_MODEL", "whisper-large-v3")

# Local Whisper configuration (used when STT_PROVIDER=local)
WHISPER_MODEL_NAME = _normalize_fw_model(os.environ.get("WHISPER_MODEL", "small"))
WHISPER_DEVICE = os.environ.get("WHISPER_DEVICE", "cpu")
WHISPER_COMPUTE = os.environ.get("WHISPER_COMPUTE_TYPE", os.environ.get("WHISPER_COMPUTE", "int8"))
WHISPER_NUM_WORKERS = int(os.environ.get("WHISPER_NUM_WORKERS", "1"))

CPU_THREADS = int(os.environ.get("WATUS_CPU_THREADS", str(os.cpu_count() or 4)))

# Piper TTS Configuration (new API compatibility)
PIPER_MODEL_PATH = os.environ.get("PIPER_MODEL_PATH", "models/piper/voices/pl_PL-darkman-medium.onnx")
PIPER_VOICE = None  # Will be initialized when using Python API
PIPER_SR = int(os.environ.get("PIPER_SAMPLE_RATE", "22050"))

# Legacy compatibility (fallback to binary)
PIPER_BIN = os.environ.get("PIPER_BIN")
PIPER_CONFIG = os.environ.get("PIPER_CONFIG")

# ===== Audio/VAD =====
SAMPLE_RATE = int(os.environ.get("WATUS_SR", "16000"))
BLOCK_SIZE = int(os.environ.get("WATUS_BLOCKSIZE", str(int(round(SAMPLE_RATE * 0.02)))))  # ~20 ms
VAD_MODE = int(os.environ.get("WATUS_VAD_MODE", "1"))
VAD_MIN_MS = int(os.environ.get("WATUS_VAD_MIN_MS", "150"))  # ZMNIEJSZONO z 280ms dla krótkich słów
SIL_MS_END = int(os.environ.get("WATUS_SIL_MS_END", "450"))  # ZMNIEJSZONO z 650ms dla szybszej reakcji
ASR_MIN_DBFS = float(os.environ.get("ASR_MIN_DBFS", "-34"))

# endpoint anti-chop
PREBUFFER_FRAMES = int(os.environ.get("WATUS_PREBUFFER_FRAMES", "15"))  # 15 ramek * 20ms = 300ms
START_MIN_FRAMES = int(os.environ.get("WATUS_START_MIN_FRAMES", "4"))  # 4 ramki * 20ms = 80ms
START_MIN_DBFS = float(os.environ.get("WATUS_START_MIN_DBFS", str(ASR_MIN_DBFS + 4.0)))
MIN_MS_BEFORE_ENDPOINT = int(os.environ.get("WATUS_MIN_MS_BEFORE_ENDPOINT", "500"))
END_AT_DBFS_DROP = float(os.environ.get("END_AT_DBFS_DROP", "0"))
EMIT_COOLDOWN_MS = int(os.environ.get("EMIT_COOLDOWN_MS", "300"))
MAX_UTT_MS = int(os.environ.get("MAX_UTT_MS", "6500"))
GAP_TOL_MS = int(os.environ.get("WATUS_GAP_TOL_MS", "450"))  # cisza, którą jeszcze tolerujemy w środku wypowiedzi

IN_DEV_ENV = os.environ.get("WATUS_INPUT_DEVICE")
OUT_DEV_ENV = os.environ.get("WATUS_OUTPUT_DEVICE")

DIALOG_PATH = os.environ.get("DIALOG_PATH", "dialog.jsonl")

# ===== Weryfikacja mówcy =====
SPEAKER_VERIFY = int(os.environ.get("SPEAKER_VERIFY", "1"))
WAKE_WORDS = [w.strip() for w in
              os.environ.get("WAKE_WORDS", "hej watusiu,hej watuszu,hej watusił,kej watusił,hej watośiu").split(",") if
              w.strip()]
SPEAKER_THRESHOLD = float(os.environ.get("SPEAKER_THRESHOLD", "0.64"))
SPEAKER_STICKY_THRESHOLD = float(os.environ.get("SPEAKER_STICKY_THRESHOLD", str(SPEAKER_THRESHOLD)))
SPEAKER_GRACE = float(os.environ.get("SPEAKER_GRACE", "0.12"))  # lekko w górę – emocje
SPEAKER_STICKY_SEC = float(os.environ.get("SPEAKER_STICKY_SEC", os.environ.get("SPEAKER_STICKY_S", "3600")))
SPEAKER_MIN_ENROLL_SCORE = float(os.environ.get("SPEAKER_MIN_ENROLL_SCORE", "0.55"))
SPEAKER_MIN_DBFS = float(os.environ.get("SPEAKER_MIN_DBFS", "-40"))
SPEAKER_MAX_DBFS = float(os.environ.get("SPEAKER_MAX_DBFS", "-5"))
SPEAKER_BACK_THRESHOLD = float(os.environ.get("SPEAKER_BACK_THRESHOLD", "0.56"))
SPEAKER_REQUIRE_MATCH = int(os.environ.get("SPEAKER_REQUIRE_MATCH", "1"))

# ===== Zachowanie =====
WAIT_REPLY_S = float(os.environ.get("WAIT_REPLY_S", "0.6"))  # max czekania na TTS zanim wrócimy do słuchania


def log(msg): print(msg, flush=True)


def is_wake_word_present(text: str) -> bool:
    """
    Sprawdza obecność słowa-klucza w bardziej elastyczny sposób.
    1. Normalizuje tekst wejściowy (małe litery, usuwa znaki interpunkcyjne).
    2. Sprawdza, czy którakolwiek z fraz kluczowych (również znormalizowana) znajduje się w tekście.
    """
    normalized_text = re.sub(r'[^\w\s]', '', text.lower())

    for wake_phrase in WAKE_WORDS:
        normalized_wake_phrase = re.sub(r'[^\w\s]', '', wake_phrase.lower())
        if normalized_wake_phrase in normalized_text:
            return True
    return False


# ===== LED Controller =====
led = LEDController()
atexit.register(led.cleanup)


def list_devices():
    try:
        devs = sd.query_devices()
        log("[Watus] Audio devices:")
        for i, d in enumerate(devs):
            log(f"  [{i}] {d['name']} in:{d.get('max_input_channels', 0)} out:{d.get('max_output_channels', 0)}")
    except Exception as e:
        log(f"[Watus] Nie można wypisać urządzeń: {e}")


def _match_device_by_name(fragment: str, want_output=False):
    frag = (fragment or "").lower().strip()
    for i, d in enumerate(sd.query_devices()):
        if frag and frag in d['name'].lower():
            if want_output and d.get("max_output_channels", 0) > 0: return i
            if not want_output and d.get("max_input_channels", 0) > 0: return i
    return None


def _coerce_dev(v, want_output=False):
    if not v: return None
    try:
        return int(v)
    except ValueError:
        return _match_device_by_name(v, want_output=want_output)


def _auto_pick_input():
    try:
        di = sd.default.device
        if isinstance(di, (list, tuple)) and di[0] is not None: return di[0]
    except Exception:
        pass
    for i, d in enumerate(sd.query_devices()):
        if d.get("max_input_channels", 0) > 0: return i
    return None


def _auto_pick_output():
    try:
        di = sd.default.device
        if isinstance(di, (list, tuple)) and di[1] is not None: return di[1]
    except Exception:
        pass
    for i, d in enumerate(sd.query_devices()):
        if d.get("max_output_channels", 0) > 0: return i
    return None


IN_DEV = _coerce_dev(IN_DEV_ENV, want_output=False) or _auto_pick_input()
OUT_DEV = _coerce_dev(OUT_DEV_ENV, want_output=True) or _auto_pick_output()


# ===== ZMQ Bus =====
class Bus:
    def __init__(self, pub_addr: str, sub_addr: str):
        self.ctx = zmq.Context.instance()

        # Create sockets
        self.pub = self.ctx.socket(zmq.PUB)
        self.pub.setsockopt(zmq.SNDHWM, 100)
        self.pub.bind(pub_addr)

        self.sub = self.ctx.socket(zmq.SUB)
        self.sub.connect(sub_addr)
        self.sub.setsockopt_string(zmq.SUBSCRIBE, "tts.speak")

        self._sub_queue = queue.Queue()

        # Track running state
        self._running = True
        self._sub_thread = threading.Thread(target=self._sub_loop, daemon=True)
        self._sub_thread.start()

        # Register cleanup
        atexit.register(self._cleanup)

    def _cleanup(self):
        """Clean up ZMQ resources properly"""
        try:
            self._running = False

            # Close sockets first
            if hasattr(self, 'pub') and self.pub:
                try:
                    self.pub.close()
                except:
                    pass

            if hasattr(self, 'sub') and self.sub:
                try:
                    self.sub.close()
                except:
                    pass

            # Note: Don't close context - it's singleton and may be used by other parts

        except Exception as e:
            print(f"[Watus][BUS] Cleanup error: {e}", file=sys.stderr)

    def publish_leader(self, payload: dict):
        try:
            t0 = time.time()
            self.pub.send_multipart([b"dialog.leader", json.dumps(payload, ensure_ascii=False).encode("utf-8")])
            log(f"[Perf] BUS_ms={int((time.time() - t0) * 1000)}")
        except Exception as e:
            log(f"[Watus][BUS] Publish leader error: {e}")

    def publish_state(self, state: str, data: dict = None):
        """Publish current watus state for real-time UI synchronization"""
        try:
            if data is None:
                data = {}
            message = {
                "state": state,
                "timestamp": time.time(),
                **data
            }
            self.pub.send_multipart([b"watus.state", json.dumps(message, ensure_ascii=False).encode("utf-8")])
        except Exception as e:
            log(f"[Watus] Failed to publish state message: {e}")

    def _sub_loop(self):
        while self._running:
            try:
                topic, payload = self.sub.recv_multipart()
                if topic != b"tts.speak": continue
                data = json.loads(payload.decode("utf-8", "ignore"))
                self._sub_queue.put(data)
            except Exception as e:
                if self._running:  # Only log errors if still running
                    time.sleep(0.01)

    def get_tts(self, timeout=0.1):
        try:
            return self._sub_queue.get(timeout=timeout)
        except queue.Empty:
            return None


# ===== Stan =====
class State:
    def __init__(self):
        self.session_id = f"live_{int(time.time())}"
        self._tts_active = False
        self._awaiting_reply = False
        self._lock = threading.Lock()
        self.tts_pending_until = 0.0
        self.waiting_reply_until = 0.0
        self.last_tts_id = None

    def set_tts(self, flag: bool):
        with self._lock:
            self._tts_active = flag

    def set_awaiting_reply(self, flag: bool):
        with self._lock:
            self._awaiting_reply = flag

    def pause_until_reply(self):
        with self._lock:
            self.waiting_reply_until = time.time() + WAIT_REPLY_S

    def is_blocked(self) -> bool:
        with self._lock:
            return (
                    self._tts_active
                    or self._awaiting_reply
                    or (time.time() < self.tts_pending_until)
                    or (time.time() < self.waiting_reply_until)
            )


def cue_listen():
    log("[Watus][STATE] LISTENING")
    led.listening()

    # Publikuj stan dla synchronizacji UI
    if 'bus' in globals():
        try:
            bus.publish_state("listening")
        except:
            pass


def cue_think():
    log("[Watus][STATE] THINKING")
    led.processing_or_speaking()

    # Publikuj stan dla synchronizacji UI
    if 'bus' in globals():
        try:
            bus.publish_state("processing")
        except:
            pass


def cue_speak():
    log("[Watus][STATE] SPEAKING")
    led.processing_or_speaking()

    # Publikuj stan dla synchronizacji UI
    if 'bus' in globals():
        try:
            bus.publish_state("speaking")
        except:
            pass


def cue_idle():
    log("[Watus][STATE] IDLE")
    led.processing_or_speaking()


# Dodaj metodę publish_state do klasy Bus


# ===== Weryfikator (ECAPA) =====
class _NoopVerifier:
    enabled = True

    def __init__(self): self._enrolled = None

    @property
    def enrolled(self): return False

    def enroll_wav(self, p): pass

    def enroll_samples(self, s, sr): pass

    def verify(self, s, sr, db): return {"enabled": False}


def _make_verifier():
    if not SPEAKER_VERIFY: return _NoopVerifier()
    try:
        import torch  # noqa
        from speechbrain.pretrained import EncoderClassifier  # noqa
    except Exception as e:
        log(f"[Watus][SPK] OFF (brak zależności): {e}")
        return _NoopVerifier()

    class _SbVerifier:
        enabled = True

        def __init__(self):
            import torch
            self.threshold = SPEAKER_THRESHOLD
            self.sticky_thr = SPEAKER_STICKY_THRESHOLD
            self.back_thr = SPEAKER_BACK_THRESHOLD
            self.grace = SPEAKER_GRACE
            self.sticky_sec = SPEAKER_STICKY_SEC
            self._clf = None
            self._device = "cuda" if torch.cuda.is_available() else "cpu"
            self._enrolled = None
            self._enroll_ts = 0.0

        @property
        def enrolled(self):
            return self._enrolled is not None

        def _ensure(self):
            from speechbrain.pretrained import EncoderClassifier
            if self._clf is None:
                self._clf = EncoderClassifier.from_hparams(
                    source="speechbrain/spkrec-ecapa-voxceleb",
                    run_opts={"device": self._device},
                    savedir="models/ecapa",
                )

        @staticmethod
        def _resample_16k(x: np.ndarray, sr: int) -> np.ndarray:
            if sr == 16000: return x.astype(np.float32)
            ratio = 16000.0 / sr
            n_out = int(round(len(x) * ratio))
            idx = np.linspace(0, len(x) - 1, num=n_out, dtype=np.float32)
            base = np.arange(len(x), dtype=np.float32)
            return np.interp(idx, base, x).astype(np.float32)

        def _embed(self, samples: np.ndarray, sr: int):
            import torch
            self._ensure()
            wav = self._resample_16k(samples, sr)
            t = torch.tensor(wav, dtype=torch.float32).unsqueeze(0)
            with torch.no_grad():
                emb = self._clf.encode_batch(t).squeeze(0).squeeze(0)
            return emb.detach().cpu().numpy().astype(np.float32)

        def enroll_samples(self, samples: np.ndarray, sr: int):
            try:
                emb = self._embed(samples, sr)
                self._enrolled = emb
                self._enroll_ts = time.time()
                log(f"[Watus][SPK] Enrolled new leader voice.")
            except Exception as e:
                log(f"[Watus][SPK] enroll err: {e}")

        def verify(self, samples: np.ndarray, sr: int, dbfs: float) -> dict:
            if self._enrolled is None:
                return {"enabled": True, "enrolled": False}
            import torch, torch.nn.functional as F
            a = self._embed(samples, sr)
            sim = float(F.cosine_similarity(
                torch.tensor(a, dtype=torch.float32).flatten(),
                torch.tensor(self._enrolled, dtype=torch.float32).flatten(), dim=0, eps=1e-8
            ).detach().cpu().item())
            now = time.time()
            age = now - self._enroll_ts
            is_leader = False
            adj_thr = (
                        self.sticky_thr - self.grace) if dbfs > -22.0 else self.sticky_thr  # emocje → głośniej → trochę łagodniej
            if age <= self.sticky_sec and sim >= adj_thr:
                is_leader = True
            elif sim >= self.threshold:
                is_leader = True
            elif sim >= self.back_thr and age <= self.sticky_sec:
                is_leader = True
            return {"enabled": True, "enrolled": True, "score": sim, "is_leader": bool(is_leader), "sticky_age_s": age}

    return _SbVerifier()


# ===== Piper CLI =====
def _env_with_libs_for_piper(piper_bin: str) -> dict:
    env = os.environ.copy()
    bin_dir = os.path.dirname(piper_bin) if piper_bin else ""
    phonemize_lib = os.path.join(bin_dir, "piper-phonemize", "lib")
    extra_paths = []
    if os.path.isdir(bin_dir): extra_paths.append(bin_dir)
    if os.path.isdir(phonemize_lib): extra_paths.append(phonemize_lib)
    if not extra_paths: return env

    if sys.platform == "darwin":
        key = "DYLD_LIBRARY_PATH"
    elif sys.platform.startswith("linux"):
        key = "LD_LIBRARY_PATH"
    else:
        key = "PATH"
    cur = env.get(key, "")
    sep = (":" if key != "PATH" else ";")
    env[key] = (sep.join([*extra_paths, cur]) if cur else sep.join(extra_paths))
    return env


# Initialize Piper voice once
def _init_piper_voice():
    global PIPER_VOICE
    if PIPER_AVAILABLE:
        if not PIPER_VOICE:  # Initialize if not already loaded
            try:
                model_path = PIPER_MODEL_PATH
                if not os.path.isfile(model_path):
                    log(f"[Watus][TTS] Brak modelu Piper: {model_path}")
                    return False
                PIPER_VOICE = PiperVoice.load(model_path)
                log(f"[Watus][TTS] Piper voice załadowany z: {model_path}")
                return True
            except Exception as e:
                log(f"[Watus][TTS] Błąd ładowania Piper voice: {e}")
                return False
        else:
            # Already loaded
            return True
    return False


def piper_say(text: str, out_dev=OUT_DEV):
    if not text or not text.strip(): return

    log(f"[Watus][TTS] Próbuję Python API (PIPER_AVAILABLE={PIPER_AVAILABLE})")

    # Try new Python API first
    if PIPER_AVAILABLE and _init_piper_voice():
        log(f"[Watus][TTS] Używam nowego Python API")
        try:
            # Generate speech using new Python API (returns AudioChunk iterator)
            import numpy as np
            from piper import AudioChunk

            audio_data = []
            for chunk in PIPER_VOICE.synthesize(text):
                if isinstance(chunk, AudioChunk):
                    chunk_data = chunk.audio_int16_array.astype(np.float32) / 32768.0
                    audio_data.append(chunk_data)
                else:
                    log(f"[Watus][TTS] Nieoczekiwany typ chunk: {type(chunk)}")

            if audio_data:
                # Combine all audio chunks
                full_audio = np.concatenate(audio_data)
                sample_rate = PIPER_VOICE.config.sample_rate
                sd.play(full_audio, sample_rate, device=out_dev, blocking=True)
                return
            else:
                log(f"[Watus][TTS] Brak danych audio z nowego API")
        except Exception as e:
            log(f"[Watus][TTS] Błąd Python API, próbuję binary fallback: {e}")

    # Fallback to binary method (legacy)
    if not PIPER_BIN or not os.path.isfile(PIPER_BIN):
        log(f"[Watus][TTS] Uwaga: brak/niepoprawny PIPER_BIN: {PIPER_BIN}");
        return
    if not PIPER_MODEL_PATH or not os.path.isfile(PIPER_MODEL_PATH):
        log(f"[Watus][TTS] Brak/niepoprawny PIPER_MODEL_PATH: {PIPER_MODEL_PATH}");
        return

    try:
        if sys.platform == "darwin":
            bin_dir = os.path.dirname(PIPER_BIN)
            subprocess.run(["xattr", "-dr", "com.apple.quarantine", bin_dir],
                           stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False)
    except Exception:
        pass

    cfg = ["--config", PIPER_CONFIG] if PIPER_CONFIG and os.path.isfile(PIPER_CONFIG) else []
    env = _env_with_libs_for_piper(PIPER_BIN)

    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        wav_path = tmp.name
    t0 = time.time()
    try:
        cmd = [PIPER_BIN, "--model", PIPER_MODEL_PATH, *cfg, "--output_file", wav_path]
        if PIPER_SR: cmd += ["--sample_rate", str(PIPER_SR)]
        subprocess.run(cmd, input=(text or "").encode("utf-8"),
                       stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True, env=env)
        data, sr = sf.read(wav_path, dtype="float32")
        sd.play(data, sr, device=out_dev, blocking=True)
    except subprocess.CalledProcessError as e:
        err = e.stderr.decode("utf-8", "ignore") if e.stderr else str(e)
        log(f"[Watus][TTS] Piper błąd (proc): {err}")
    except Exception as e:
        log(f"[Watus][TTS] Odtwarzanie nieudane: {e}")
    finally:
        try:
            os.unlink(wav_path)
        except Exception:
            pass
        log(f"[Perf] TTS_play_ms={int((time.time() - t0) * 1000)}")


# Gemini TTS functions
def convert_to_wav(audio_data: bytes, mime_type: str) -> bytes:
    """Generates a WAV file header for the given audio data and parameters.

    Args:
        audio_data: The raw audio data as a bytes object.
        mime_type: Mime type of the audio data.

    Returns:
        A bytes object representing the WAV file header.
    """
    parameters = parse_audio_mime_type(mime_type)
    bits_per_sample = parameters["bits_per_sample"]
    sample_rate = parameters["rate"]
    num_channels = 1
    data_size = len(audio_data)
    bytes_per_sample = bits_per_sample // 8
    block_align = num_channels * bytes_per_sample
    byte_rate = sample_rate * block_align
    chunk_size = 36 + data_size  # 36 bytes for header fields before data chunk size

    # http://soundfile.sapp.org/doc/WaveFormat/

    header = struct.pack(
        "<4sI4s4sIHHIIHH4sI",
        b"RIFF",          # ChunkID
        chunk_size,       # ChunkSize (total file size - 8 bytes)
        b"WAVE",          # Format
        b"fmt ",          # Subchunk1ID
        16,               # Subchunk1Size (16 for PCM)
        1,                # AudioFormat (1 for PCM)
        num_channels,     # NumChannels
        sample_rate,      # SampleRate
        byte_rate,        # ByteRate
        block_align,      # BlockAlign
        bits_per_sample,  # BitsPerSample
        b"data",          # Subchunk2ID
        data_size         # Subchunk2Size (size of audio data)
    )
    return header + audio_data


def parse_audio_mime_type(mime_type: str) -> dict[str, int | None]:
    """Parses bits per sample and rate from an audio MIME type string.

    Assumes bits per sample is encoded like "L16" and rate as "rate=xxxxx".

    Args:
        mime_type: The audio MIME type string (e.g., "audio/L16;rate=24000").

    Returns:
        A dictionary with "bits_per_sample" and "rate" keys. Values will be
        integers if found, otherwise None.
    """
    bits_per_sample = 16
    rate = 24000

    # Extract rate from parameters
    parts = mime_type.split(";")
    for param in parts: # Skip the main type part
        param = param.strip()
        if param.lower().startswith("rate="):
            try:
                rate_str = param.split("=", 1)[1]
                rate = int(rate_str)
            except (ValueError, IndexError):
                # Handle cases like "rate=" with no value or non-integer value
                pass # Keep rate as default
        elif param.startswith("audio/L"):
            try:
                bits_per_sample = int(param.split("L", 1)[1])
            except (ValueError, IndexError):
                pass # Keep bits_per_sample as default if conversion fails

    return {"bits_per_sample": bits_per_sample, "rate": rate}


def gemini_say(text: str, out_dev=OUT_DEV):
    """Generate speech using Gemini TTS and play it"""
    if not text or not text.strip():
        return

    if not GEMINI_AVAILABLE:
        log("[Watus][TTS] Gemini TTS not available")
        return

    if not GEMINI_API_KEY:
        log("[Watus][TTS] GEMINI_API_KEY not set")
        return

    try:
        log(f"[Watus][TTS] Generating Gemini TTS for: {text[:50]}...")

        client = genai.Client(api_key=GEMINI_API_KEY)

        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=text),
                ],
            ),
        ]

        generate_content_config = types.GenerateContentConfig(
            temperature=1,
            response_modalities=["audio"],
            speech_config=types.SpeechConfig(
                voice_config=types.VoiceConfig(
                    prebuilt_voice_config=types.PrebuiltVoiceConfig(
                        voice_name=GEMINI_VOICE
                    )
                )
            ),
        )

        t0 = time.time()

        audio_data = b""
        for chunk in client.models.generate_content_stream(
            model=GEMINI_MODEL,
            contents=contents,
            config=generate_content_config,
        ):
            if (
                chunk.candidates is None
                or chunk.candidates[0].content is None
                or chunk.candidates[0].content.parts is None
            ):
                continue
            if chunk.candidates[0].content.parts[0].inline_data and chunk.candidates[0].content.parts[0].inline_data.data:
                inline_data = chunk.candidates[0].content.parts[0].inline_data
                audio_data = inline_data.data
                mime_type = inline_data.mime_type
                break
            else:
                # Fallback if no audio data but text response
                if hasattr(chunk, 'text') and chunk.text:
                    log(f"[Watus][TTS] No audio, got text: {chunk.text}")

        if audio_data:
            # Convert to WAV format
            if mime_type and mime_type.startswith("audio/"):
                wav_data = convert_to_wav(audio_data, mime_type)
            else:
                log("[Watus][TTS] Unsupported audio format")
                return

            # Load and play audio
            import io
            wav_buffer = io.BytesIO(wav_data)
            audio_array, sample_rate = sf.read(wav_buffer, dtype='float32')

            sd.play(audio_array, sample_rate, device=out_dev, blocking=True)

            log(f"[Perf] Gemini_TTS_play_ms={int((time.time() - t0) * 1000)}")
        else:
            log("[Watus][TTS] No audio data generated by Gemini")

    except Exception as e:
        log(f"[Watus][TTS] Gemini TTS error: {e}")


# Universal TTS function with provider switching
def tts_say(text: str, out_dev=OUT_DEV):
    """Universal TTS function - Piper or Gemini based on TTS_PROVIDER"""

    if TTS_PROVIDER == "gemini":
        log("[Watus][TTS] Using Gemini TTS")
        gemini_say(text, out_dev)
    else:
        log("[Watus][TTS] Using Piper TTS")
        piper_say(text, out_dev)


# ===== JSONL =====
def append_dialog_line(obj: dict, path=DIALOG_PATH):
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")


# ===== STT =====
class STTEngine:
    def __init__(self, state: 'State', bus: 'Bus'):
        self.state = state
        self.bus = bus
        self.vad = webrtcvad.Vad(VAD_MODE)
        self.stt_provider = STT_PROVIDER

        log(f"[Watus] STT ready (device={IN_DEV} sr={SAMPLE_RATE} block={BLOCK_SIZE})")

        # Initialize STT based on provider
        if self.stt_provider == "groq":
            self._init_groq_stt()
        else:
            self._init_local_whisper()

        self.verifier = _make_verifier()
        self.emit_cooldown_ms = EMIT_COOLDOWN_MS
        self.cooldown_until = 0

    def _init_groq_stt(self):
        """Initialize Groq Speech-to-Text API"""
        if not GROQ_API_KEY:
            log("[Watus] ERROR: GROQ_API_KEY not provided, falling back to local Whisper")
            return self._init_local_whisper()

        try:
            log(f"[Watus] Groq STT init: model={GROQ_MODEL}")
            t0 = time.time()
            self.model = GroqSTT(GROQ_API_KEY, GROQ_MODEL)

            # Validate API key
            if not self.model.validate_api_key(GROQ_API_KEY):
                log("[Watus] ERROR: Invalid GROQ_API_KEY, falling back to local Whisper")
                return self._init_local_whisper()

            log(f"[Watus] STT Groq API loaded ({int((time.time() - t0) * 1000)} ms)")

        except Exception as e:
            log(f"[Watus] ERROR: Failed to initialize Groq STT: {e}")
            log("[Watus] Falling back to local Whisper")
            return self._init_local_whisper()

    def _init_local_whisper(self):
        """Initialize local Faster-Whisper"""
        log(f"[Watus] FasterWhisper init: model={WHISPER_MODEL_NAME} device={WHISPER_DEVICE} "
            f"compute={WHISPER_COMPUTE} cpu_threads={CPU_THREADS} workers={WHISPER_NUM_WORKERS}")
        t0 = time.time()
        self.model = WhisperModel(
            WHISPER_MODEL_NAME,
            device=WHISPER_DEVICE,
            compute_type=WHISPER_COMPUTE,
            cpu_threads=CPU_THREADS,
            num_workers=WHISPER_NUM_WORKERS
        )
        log(f"[Watus] STT FasterWhisper loaded ({int((time.time() - t0) * 1000)} ms)")
        self.stt_provider = "local"

    @staticmethod
    def _rms_dbfs(x: np.ndarray, eps=1e-9):
        rms = np.sqrt(np.mean(np.square(x) + eps))
        return 20 * np.log10(max(rms, eps))

    def _vad_is_speech(self, frame_bytes: bytes) -> bool:
        try:
            return self.vad.is_speech(frame_bytes, SAMPLE_RATE)
        except Exception:
            return False

    def _transcribe_float32(self, pcm_f32: np.ndarray) -> str:
        t0 = time.time()

        if self.stt_provider == "groq":
            # Use Groq API for transcription
            try:
                txt = self.model.transcribe_numpy(pcm_f32, SAMPLE_RATE, "pl")
                log(f"[Perf] ASR_groq_ms={int((time.time() - t0) * 1000)} len={len(txt)}")
                return txt
            except Exception as e:
                log(f"[Watus] Groq transcription failed: {e}")
                log("[Watus] Falling back to local Whisper")
                return self._transcribe_local(pcm_f32)
        else:
            # Use local Faster-Whisper
            return self._transcribe_local(pcm_f32)

    def _transcribe_local(self, pcm_f32: np.ndarray) -> str:
        """Transcribe using local Faster-Whisper"""
        t0 = time.time()
        segments, _ = self.model.transcribe(
            pcm_f32, language="pl", beam_size=1, vad_filter=False
        )
        txt = "".join(seg.text for seg in segments)
        log(f"[Perf] ASR_local_ms={int((time.time() - t0) * 1000)} len={len(txt)}")
        return txt

    def run(self):
        in_stream = sd.InputStream(
            samplerate=SAMPLE_RATE, channels=1, dtype="int16",
            blocksize=BLOCK_SIZE, device=IN_DEV
        )

        frame_ms = int(1000 * BLOCK_SIZE / SAMPLE_RATE)
        sil_frames_end = max(1, SIL_MS_END // frame_ms)
        min_speech_frames = max(1, VAD_MIN_MS // frame_ms)

        pre_buffer = deque(maxlen=PREBUFFER_FRAMES)
        speech_frames = bytearray()
        in_speech = False
        started_ms = None
        last_voice_ms = 0
        listening_flag = None

        # start detection
        start_voice_run = 0

        # for anti-drop
        last_dbfs = None
        speech_frames_count = 0
        silence_run = 0
        gap_run = 0

        with in_stream:
            while True:
                now_ms = int(time.time() * 1000)

                if self.state.is_blocked():
                    if listening_flag is not False:
                        cue_idle();
                        listening_flag = False
                    in_speech = False;
                    speech_frames = bytearray();
                    started_ms = None
                    start_voice_run = 0;
                    last_dbfs = None
                    speech_frames_count = 0;
                    silence_run = 0;
                    gap_run = 0
                    pre_buffer.clear()
                    time.sleep(0.01);
                    continue

                if now_ms < self.cooldown_until:
                    time.sleep(0.003);
                    continue

                if listening_flag is not True:
                    cue_listen();
                    listening_flag = True

                try:
                    audio, _ = in_stream.read(BLOCK_SIZE)
                except Exception as e:
                    log(f"[Watus][STT] read err: {e}")
                    time.sleep(0.01);
                    continue

                frame_bytes = audio.tobytes()
                pre_buffer.append(frame_bytes)
                is_sp = self._vad_is_speech(frame_bytes)

                # --- twardy start: kilka ramek powyżej progu dBFS ---
                if not in_speech:
                    if is_sp:
                        cur = np.frombuffer(frame_bytes, dtype=np.int16).astype(np.float32) / 32768.0
                        cur_db = float(self._rms_dbfs(cur))
                        if cur_db > START_MIN_DBFS:
                            start_voice_run += 1
                        else:
                            start_voice_run = 0
                        if start_voice_run >= START_MIN_FRAMES:
                            in_speech = True
                            speech_frames = bytearray()
                            if pre_buffer:
                                speech_frames.extend(b''.join(pre_buffer))
                            started_ms = now_ms - (len(pre_buffer) * frame_ms)
                            last_voice_ms = now_ms
                            speech_frames_count = 0
                            silence_run = 0
                            gap_run = 0
                            last_dbfs = None
                            start_voice_run = 0
                    else:
                        start_voice_run = 0
                    time.sleep(0.0005)
                    continue

                # --- w turze mowy ---
                if is_sp:
                    speech_frames.extend(frame_bytes)
                    last_voice_ms = now_ms
                    silence_run = 0
                    gap_run = 0
                    speech_frames_count += 1

                    cur = np.frombuffer(frame_bytes, dtype=np.int16).astype(np.float32) / 32768.0
                    cur_db = float(self._rms_dbfs(cur))
                    if last_dbfs is None: last_dbfs = cur_db

                    if END_AT_DBFS_DROP > 0:
                        if speech_frames_count >= min_speech_frames and (
                                now_ms - (started_ms or now_ms)) >= MIN_MS_BEFORE_ENDPOINT:
                            if (last_dbfs - cur_db) >= END_AT_DBFS_DROP:
                                in_speech = False
                                dur_ms = last_voice_ms - (started_ms or last_voice_ms)
                                self._finalize(speech_frames, started_ms, last_voice_ms, dur_ms)
                                listening_flag = None
                                speech_frames = bytearray();
                                started_ms = None
                                speech_frames_count = 0;
                                silence_run = 0;
                                last_dbfs = None;
                                gap_run = 0
                                self.cooldown_until = now_ms + self.emit_cooldown_ms
                                continue
                    else:
                        last_dbfs = cur_db

                else:
                    # brak VAD -> liczymy ciszę i tolerowany GAP
                    silence_run += 1
                    gap_run += frame_ms
                    # toleruj krótką przerwę w środku wypowiedzi
                    if gap_run < GAP_TOL_MS:
                        speech_frames.extend(frame_bytes)  # dodajemy ciszę do bufora na wszelki wypadek
                        continue

                    if silence_run >= sil_frames_end and (now_ms - (started_ms or now_ms)) >= MIN_MS_BEFORE_ENDPOINT:
                        in_speech = False
                        dur_ms = last_voice_ms - (started_ms or last_voice_ms)
                        if dur_ms >= VAD_MIN_MS and len(speech_frames) > 0:
                            self._finalize(speech_frames, started_ms, last_voice_ms, dur_ms)
                            self.cooldown_until = now_ms + self.emit_cooldown_ms
                        listening_flag = None
                        speech_frames = bytearray();
                        started_ms = None
                        speech_frames_count = 0;
                        silence_run = 0;
                        last_dbfs = None;
                        gap_run = 0

                # twardy limit
                if in_speech and started_ms and (now_ms - started_ms) >= MAX_UTT_MS:
                    in_speech = False
                    dur_ms = last_voice_ms - (started_ms or last_voice_ms)
                    if dur_ms >= VAD_MIN_MS and len(speech_frames) > 0:
                        self._finalize(speech_frames, started_ms, last_voice_ms, dur_ms)
                        self.cooldown_until = now_ms + self.emit_cooldown_ms
                    listening_flag = None
                    speech_frames = bytearray();
                    started_ms = None
                    speech_frames_count = 0;
                    silence_run = 0;
                    last_dbfs = None;
                    gap_run = 0

                time.sleep(0.0005)

    def _finalize(self, speech_frames: bytearray, started_ms: int, last_voice_ms: int, dur_ms: int):
        cue_think()
        pcm_f32 = np.frombuffer(speech_frames, dtype=np.int16).astype(np.float32) / 32768.0
        dbfs = float(self._rms_dbfs(pcm_f32))
        if dbfs < ASR_MIN_DBFS:
            return

        # 1. Transkrypcja
        text = self._transcribe_float32(pcm_f32).strip()
        if not text:
            return

        # 2. Logika Lidera oparta na słowie-klucz
        verify = {}
        is_leader = False
        is_wake_word = is_wake_word_present(text)

        if getattr(self.verifier, "enabled", False):
            if is_wake_word:
                log("[Watus][SPK] Wykryto słowo-klucz. Rejestrowanie nowego lidera.")
                self.verifier.enroll_samples(pcm_f32, SAMPLE_RATE)
                verify = self.verifier.verify(pcm_f32, SAMPLE_RATE, dbfs)
                is_leader = True
            elif self.verifier.enrolled:
                verify = self.verifier.verify(pcm_f32, SAMPLE_RATE, dbfs)
                is_leader = bool(verify.get("is_leader", False))
            else:
                log(f"[Watus][SPK] Brak lidera i słowa-klucz. Ignoruję: '{text}'")
                return
        else:
            # Jeśli weryfikacja jest wyłączona, każda wypowiedź jest od "lidera"
            is_leader = not SPEAKER_REQUIRE_MATCH

        # 3. Przygotowanie i wysłanie danych
        ts_start = (started_ms or last_voice_ms) / 1000.0
        ts_end = last_voice_ms / 1000.0
        turn_id = int(last_voice_ms)

        line = {
            "type": "leader_utterance" if is_leader else "unknown_utterance",
            "session_id": self.state.session_id,
            "group_id": f"{'leader' if is_leader else 'unknown'}_{turn_id}",
            "speaker_id": "leader" if is_leader else "unknown",
            "is_leader": is_leader,
            "turn_ids": [turn_id],
            "text_full": text,
            "category": "wypowiedź",
            "reply_hint": is_leader,
            "ts_start": ts_start,
            "ts_end": ts_end,
            "dbfs": dbfs,
            "verify": verify,
            "emit_reason": "endpoint",
            "ts": time.time()
        }
        append_dialog_line(line, DIALOG_PATH)

        if is_leader:
            log(f"[Watus][PUB] dialog.leader → group={line['group_id']} spk_score={verify.get('score')}")
            self.state.set_awaiting_reply(True)
            self.bus.publish_leader(line)
            self.state.pause_until_reply()
            self.state.tts_pending_until = time.time() + 0.6
        else:
            log(f"[Watus][SKIP] unknown (score={verify.get('score', 0):.2f}) zapisany, nie wysyłam ZMQ")


# ===== TTS worker =====
def tts_worker(state: State, bus: Bus):
    log("[Watus] Piper ready.")
    while True:
        msg = bus.get_tts(timeout=0.1)
        if not msg: continue
        text = (msg.get("text") or "").strip()
        reply_to = msg.get("reply_to") or ""
        if state.last_tts_id == reply_to and reply_to:
            log(f"[Watus][SUB] tts.speak DUP reply_to={reply_to} – pomijam")
            continue
        state.last_tts_id = reply_to

        if text:
            log(f"[Watus][LLM] answer len={len(text)} (reply_to={reply_to})")

        # OD TERAZ prawdziwy TTS – blokujemy słuchanie
        state.set_awaiting_reply(False)
        state.set_tts(True);
        cue_speak()
        try:
            tts_say(text, out_dev=OUT_DEV)
        finally:
            state.set_tts(False);
            cue_listen()


# ===== Main =====
if __name__ == "__main__":
    log(f"[Env] ASR=Faster WHISPER_MODEL={WHISPER_MODEL_NAME} WHISPER_DEVICE={WHISPER_DEVICE} "
        f"WHISPER_COMPUTE={WHISPER_COMPUTE} WATUS_BLOCKSIZE={BLOCK_SIZE}")
    log(f"[Watus] Wake words: {WAKE_WORDS}")
    log(f"[Watus] PUB dialog.leader @ {PUB_ADDR} | SUB tts.speak @ {SUB_ADDR}")
    list_devices()
    bus = Bus(PUB_ADDR, SUB_ADDR)
    state = State()
    threading.Thread(target=tts_worker, args=(state, bus), daemon=True).start()

    try:
        stt = STTEngine(state, bus)
    except Exception as e:
        log(f"[Watus] STT init error: {e}");
        sys.exit(1)

    log(f"[Watus] IO: input={IN_DEV!r} | output={OUT_DEV!r}")
    led.listening()  # Start with listening state
    try:
        stt.run()
    except KeyboardInterrupt:
        log("[Watus] stop");
        sys.exit(0)
    except Exception as e:
        import traceback;

        traceback.print_exc()
        log(f"[Watus] fatal: {e}");
        sys.exit(1)
